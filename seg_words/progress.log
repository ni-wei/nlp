Task: 
1. to use jieba to segment the GuiZhou H reports into words, and, 
2. to generate a user dictionary using those words.

========================================

Input corpus: Column "影像学表现" and Column "影像学诊断" of the 67 .xlsx files in the directory /home/idp/idpCHN/word_seg/iter_input

!These 67 .xlsx files are NOT all the data we copied from the GuiZhou H.!
!See all the data in /home/idp/idpCHN/word_seg/complete_till26thMar2018!

excel_to_txt.py generated report_in_txt.txt from the input corpus. excel_to_txt.py is not well structure and should be reviewed.

report_in_txt.txt is of the size 438.9MB, and is not located here but at in the directory /home/idp/idpCHN/word_seg

!report_in_txt.txt is the input of the subsequent procedure.!
--------
# Note made on 23.04.18:
When generating v0.2 dict using v0.1 dict, i modified a few lines in report_in_txt.txt, e.g., deleting those obvious typos/errors.
--------

========================================

## word_cut.py generated new_dict.txt from report_in_txt.txt.
## Renamed new_dict.txt as dict_sortedByFreq.txt, and hence cannot be found in this dir.
## NB: due to unsuccessful version control of word_cut.py, I cannot repeat the 2 steps described in the above 2 lines any more.
## Hence these 2 steps will be replaced by: 
word_cut.py took report_in_txt.txt as the input, and generated test_dict.txt.
(word_cut.py includes a method excluding the number words/基数词 from the output test_dict.txt. I.e., it prints into the output file only those words that are NOT numbers)

========================================

Deleted all words whose freq. <3 from test_dict.txt. 

Also removed the freq. numbers, using either print_word_remove_freqNum.py or less_robust_remove_freqNum.py. 

The resultant file is print_word_only.txt.

========================================
!High importance!
## Need to append Part II of the old dict (e.g., "coronary_dict.txt") to the end of this new dict (e.g., "GuiZhou_dict_sorted_v0.txt").
## Reason is that jieba used "coronary_dict.txt" to generate the "GuiZhou_dict_sorted_v0.txt" dict, 
## therefore those words Part II of "coronary_dict.txt" won't appear in the jieba-generated GuiZhou dict, 
## and that we need to manually add them into the new/final dict.

========================================
------------
temp note. probably made on Apr 13: 
Line 16126 - 18413: freq. == 3
Line 14419 - 16125: freq. == 4
------------
Manually correct print_word_only.txt. On going.
Update: first round of manual correction is done. 

Time consumed in hours (both task and time are approx./not accurately logged):
			last 1000 words			8 (Apr 2 - 3)
			1st 1000 words			3 (Apr 3 PM)
			2nd 1000 words			7 (Apr 4 9am - 5pm excluding 1 hr lunch time)
			3rd 1000 words			4 (Apr 5-7, OT 2 hrs, Apr 8, 8:45-10:45)
			4th 1000 words			6 (Apr 8-9, ard 3 hrs each day)
			5th 1000 words			6 (Apr 10-12, ard 2 hrs each day)
			6th 1000 words			5 (Apr 12-13, 2 + 3 hrs)
			7th 1000 words			5 (Apr 13)
			8th 1000 words			5 (Apr 14)
			9th 1000 words			5 (Apr 15)
			10th 1000 words			5 (Apr 16)
(Cunxin)	11th 1000 words			6 (Apr 16-17)
(Xu Jing)	12th 1000 words			6 (Apr 16-17)
			13th 1000 words			7 (Apr 17)
			14th 1000 words	+ ~500	8 (Apr 18-19)

========================================

###This section is to-be-reviewed. My method for Task 1 keeps changing! 
###The current method is word_cut.py 

method for Task 1:
## !Not! combine outputs using both jieba.lcut full mode ##
## and jieba.lcut default (precise) mode ##
## as a set and output to output_brain_finding.txt ##

- jieba.lcut default (precise) mode.
- output as a set, so that there is no duplicate items in the output set.
- output to finding_n_diag.txt.

========================================

When the manual correction is done, use print_sort.py to post-process print_word_only.txt.
print_sort.py simply split the input file into two output files: 1 for the correct words and 1 for the incorrect ones.
The rule of splitting merely depends on that an incorrect word should have a '0' at the line end. NB: this is NOT robust! need to enhance.

One should also combine the two output files into a single user dictionary. Ideally one should also programmatically do this combination job. For the time being, it is manually done.

(perhaps a more important note) NB: Besides the two files, in my opinion, one should also select those incorrect words in the old user dictionaries, i.e. those words denoted by a '0' at the line end. Then one should add them into the jieba cut result/new user dictionary. 
The reason is that those incorrect words in the old user dictionaries will definitely not appear in the jieba cut result this round, but that they will be useful the next time we use jieba cut. Hence they should be added into the new user dictionary.

========================================

			time (in secs) to process all 67 medical reports i have, using jieba.enable_parallel(4):
Process 1.	708.8075230121613 (parallel)
Process 2.	693.5249302387238 (parallel)
Process 3.	701.7814025878906 (no parallel)

