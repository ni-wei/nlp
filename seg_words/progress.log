Task: 
1. to use jieba to segment the GuiZhou H reports into words, and, 
2. to generate a user dictionary using those words.

========================================

Input corpus: Column "影像学表现" and Column "影像学诊断" of the 67 .xlsx files in the directory /home/idp/idpCHN/word_seg/iter_input

!These 67 .xlsx files are NOT all the data we copied from the GuiZhou H.!
!See all the data in /home/idp/idpCHN/word_seg/complete_till26thMar2018!

excel_to_txt.py generated report_in_txt.txt from the input corpus. excel_to_txt.py is not well structure and should be reviewed.

report_in_txt.txt is of the size 438.9MB, and is not located here but at in the directory /home/idp/idpCHN/word_seg

!report_in_txt.txt is the input of the subsequent procedure.!
--------
# Note made on 23.04.18:
When generating v0.2 dict using v0.1 dict, i modified a few lines in report_in_txt.txt, e.g., deleting those obvious typos/errors.
--------

========================================

## word_cut.py generated new_dict.txt from report_in_txt.txt.
## Renamed new_dict.txt as dict_sortedByFreq.txt, and hence cannot be found in this dir.
## NB: due to unsuccessful version control of word_cut.py, I cannot repeat the 2 steps described in the above 2 lines any more.
## Hence these 2 steps will be replaced by: 
word_cut.py took report_in_txt.txt as the input, and generated test_dict.txt.
(word_cut.py includes a method excluding the number words/基数词 from the output test_dict.txt. I.e., it prints into the output file only those words that are NOT numbers)

========================================

Deleted all words whose freq. <3 from test_dict.txt. 

Also removed the freq. numbers, using either print_word_remove_freqNum.py or less_robust_remove_freqNum.py. 

The resultant file is print_word_only.txt.

========================================
!High importance!
## Need to append Part II of the old dict (e.g., "coronary_dict.txt") to the end of this new dict (e.g., "GuiZhou_dict_sorted_v0.txt").
## Reason is that jieba used "coronary_dict.txt" to generate the "GuiZhou_dict_sorted_v0.txt" dict, 
## therefore those words Part II of "coronary_dict.txt" won't appear in the jieba-generated GuiZhou dict, 
## and that we need to manually add them into the new/final dict.

========================================
------------
temp note. probably made on Apr 13: 
Line 16126 - 18413: freq. == 3
Line 14419 - 16125: freq. == 4
------------
Manually correct print_word_only.txt. On going.
Update: first round of manual correction is done. 

Time consumed in hours (both task and time are approx./not accurately logged):
			last 1000 words			8 (Apr 2 - 3)
			1st 1000 words			3 (Apr 3 PM)
			2nd 1000 words			7 (Apr 4 9am - 5pm excluding 1 hr lunch time)
			3rd 1000 words			4 (Apr 5-7, OT 2 hrs, Apr 8, 8:45-10:45)
			4th 1000 words			6 (Apr 8-9, ard 3 hrs each day)
			5th 1000 words			6 (Apr 10-12, ard 2 hrs each day)
			6th 1000 words			5 (Apr 12-13, 2 + 3 hrs)
			7th 1000 words			5 (Apr 13)
			8th 1000 words			5 (Apr 14)
			9th 1000 words			5 (Apr 15)
			10th 1000 words			5 (Apr 16)
(Cunxin)	11th 1000 words			6 (Apr 16-17)
(Xu Jing)	12th 1000 words			6 (Apr 16-17)
			13th 1000 words			7 (Apr 17)
			14th 1000 words	+ ~500	8 (Apr 18-19)

========================================

###This section is to-be-reviewed. My method for Task 1 keeps changing! 
###The current method is word_cut.py 

method for Task 1:
## !Not! combine outputs using both jieba.lcut full mode ##
## and jieba.lcut default (precise) mode ##
## as a set and output to output_brain_finding.txt ##

- jieba.lcut default (precise) mode.
- output as a set, so that there is no duplicate items in the output set.
- output to finding_n_diag.txt.

========================================

When the manual correction is done, use print_sort.py to post-process print_word_only.txt.
print_sort.py simply split the input file into two output files: 1 for the correct words and 1 for the incorrect ones.
The rule of splitting merely depends on that an incorrect word should have a '0' at the line end. NB: this is NOT robust! need to enhance.

One should also combine the two output files into a single user dictionary. Ideally one should also programmatically do this combination job. For the time being, it is manually done.

(perhaps a more important note) NB: Besides the two files, in my opinion, one should also select those incorrect words in the old user dictionaries, i.e. those words denoted by a '0' at the line end. Then one should add them into the jieba cut result/new user dictionary. 
The reason is that those incorrect words in the old user dictionaries will definitely not appear in the jieba cut result this round, but that they will be useful the next time we use jieba cut. Hence they should be added into the new user dictionary.

========================================

			time (in secs) to process all 67 medical reports i have, using jieba.enable_parallel(4):
Process 1.	708.8075230121613 (parallel)
Process 2.	693.5249302387238 (parallel)
Process 3.	701.7814025878906 (no parallel)

========================================

!Completed: correct the "freq_noLessThan5_dict_v1.txt" user dict!
Find errors using the coronary text (~/Desktop/nw_dictionary/tttttttttttt.txt) and the "freq_noLessThan5_dict_v1.txt" user dict.
This is to correct word-seg errors in the coronary text/to guarantee the word-seg results is error-free for the coronary text.

Besides the corrections listed below, i corrected a few more entries, such as 野呈细 and 及结. !The corrections is all what "freq_noLessThan5_dict_v2.txt" differs from "freq_noLessThan5_dict_v1.txt".!

强灶：检查自定义词典（增加"增强灶"） 
余肝：自定义词典错误。删除。
左右冠状动脉: exists in my output.
干横径：自定义词典错误。删除。
锐缘：检查自定义词典（增加"锐缘支"） 
旁及：自定义词典错误。删除。
平胸：自定义词典错误。删除。
冠区：自定义词典错误（增加"放射冠"、"放射冠区"）

单字词：重新检查？
啊：保留
主动：保留
下线：保留
腘：保留；并增加"腘窝"
后见：删除
沟内：删除。增加"房室沟"、"结肠沟"
成角：保留

于表浅 0
及缘 0
于升 0
及无冠 0
瘘入 0
起远 0
及略 0
及锐 0
时见 0
窦回 0
左势 0
最处
成约 0
处仅 0
余娥 0
根桥 0
近及 0
技及 0
各心 0
以非化 0
引引 0
形无肺 0
近中 0
主肝见 0
支自 0
日胸 0
以远后 0
此桥 0
近长 0
见多出 0
点见 0
为毫 0
见睛 0
面内 0
及以 0
见小经 0
参数均 0
无冠 0
因以 0
管余 0
升及 0
处窄 0
于主 0

========================================

freq_noLessThan5_dict_v3.txt is the result of further modifying freq_noLessThan5_dict_v2.txt. 
v3 is the latest version.

The command and difference between these 2 versions can be seen as follows. Only 2 changes.

diff freq_noLessThan5_dict_v2.txt freq_noLessThan5_dict_v3.txt 
2391c2391
< 起
---
> 起 0
4790c4790
< 引
---
> 引 0

========================================

Experiment: change word frequency in user dict.

Set-up: change Line 978 of "freq_noLessThan5_dict_v3.txt",
from "升主动脉" to "升主动脉 4" (omitting the new line character '\n' at the end of the line).

Result: consequent difference in jieba word-seg as follows. Illustrates how word frequency in user dict influences the word-seg result.

25998,25999c25998,25999
< 升
< 主动脉壁
---
> 升主动脉
> 壁
52977,52978c52977,52978
< 升
< 主动脉壁
---
> 升主动脉
> 壁
69046,69047c69046,69047
< 升
< 主动脉壁
---
> 升主动脉
> 壁
70307,70308c70307,70308
< 升
< 主动脉壁
---
> 升主动脉
> 壁
212053,212054c212053,212054
< 升
< 主动脉壁
---
> 升主动脉
> 壁

